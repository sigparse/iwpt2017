@InProceedings{liu-zhang:2017:IWPT,
  author    = {Liu, Jiangming  and  Zhang, Yue},
  title     = {Encoder-Decoder Shift-Reduce Syntactic Parsing},
  booktitle = {Proceedings of the 15th International Conference on Parsing Technologies},
  month     = {September},
  year      = {2017},
  address   = {Pisa, Italy},
  publisher = {Association for Computational Linguistics},
  pages     = {105--114},
  abstract  = {Encoder-decoder neural networks have been used for many NLP tasks, such as
	neural machine translation. They have also been applied to constituent parsing
	by using bracketed tree structures as a target language, translating input
	sentences into syntactic trees. A more commonly used method to linearize
	syntactic trees is the shift-reduce system, which uses a sequence of
	transition-actions to build trees. We empirically investigate the effectiveness
	of applying the encoder-decoder network to transition-based parsing. On
	standard benchmarks, our system gives comparable results to the stack LSTM
	parser for dependency parsing, and significantly better results compared to the
	aforementioned parser for constituent parsing, which uses bracketed tree
	formats.},
  url       = {http://www.aclweb.org/anthology/W17-6315}
}

